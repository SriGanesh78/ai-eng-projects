{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0fb30d",
   "metadata": {},
   "source": [
    "# Project 4: **Build a Deep Research System**\n",
    "Welcome to project 4! For this project, we shift our focus from tool use and agents to *reasoning* models. You will practice state‑of‑the‑art inference‑time scaling methods such as *Chain‑of‑Thought* prompting and *Tree‑of‑Thoughts*, and briefly explore high-levels of training reasoning models using techniques like **STaR**.\n",
    "\n",
    "\n",
    "Finally, you will put everything together to build a *deep research agent* that can browse the web, reason over what it finds, and give structured answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54845369",
   "metadata": {},
   "source": [
    "## Learning Objectives  \n",
    "* Apply common inference‑time scaling methods: **zero‑shot / few‑shot CoT, self‑consistency, sequential decoding, tree‑of‑thoughts**  \n",
    "* Gain intuition for **training** reasoning‑capable models following **STaR** approach \n",
    "* Build a minimal **deep‑research agent** that combines step‑by‑step reasoning with live web search   \n",
    "* Practice extending deep-search to a multi-agent system "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a40a86",
   "metadata": {},
   "source": [
    "## Roadmap  \n",
    "1. Environment setup  \n",
    "2. Inference‑time scaling  \n",
    "   2.1 Few‑shot & zero‑shot CoT  \n",
    "   2.2 Self‑consistency\n",
    "   2.3 Sequential revisions  \n",
    "   2.4 Tree‑of‑Thought\n",
    "3. STaR for training models for reasoning  \n",
    "4. Deep-research agent  \n",
    "5. (Optional) Multi-agent deep-research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e480f76",
   "metadata": {},
   "source": [
    "# 1‑ Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c2218",
   "metadata": {},
   "source": [
    "## 1.1- Conda environment\n",
    "\n",
    "Before we start coding, you need a reproducible setup. Open a terminal in the same directory as this notebook and run:\n",
    "\n",
    "```bash\n",
    "# Create and activate the conda environment\n",
    "conda env create -f environment.yaml && conda activate deep_research\n",
    "\n",
    "# Register this environment as a Jupyter kernel\n",
    "python -m ipykernel install --user --name=deep_research --display-name \"deep_research\"\n",
    "```\n",
    "Once this is done, you can select \"deep_research\" from the Kernel → Change Kernel menu in Jupyter or VS Code.\n",
    "\n",
    "## 1.2 Ollama setup\n",
    "\n",
    "In this project we use the `llama3.2:3b` and `deepseek-r1:8b` models. You can try other smaller or larger reasoning LLMs such as `qwen2.5:3b-instruct` or `phi4-mini` to compare performance. Explore available models here: https://ollama.com/library.\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.2:3b\n",
    "ollama pull deepseek-r1:8b\n",
    "# Additional small reasoning models to compare\n",
    "# ollama pull qwen2.5:3b-instruct\n",
    "# ollama pull phi4-mini\n",
    "\n",
    "```\n",
    "\n",
    "`ollama pull` downloads the model so you can run it locally without API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8d1b7",
   "metadata": {},
   "source": [
    "---  \n",
    "# 2‑ Inference‑time scaling\n",
    "\n",
    "Inference-time scaling refers to techniques that make an existing model reason better without retraining it. Instead of changing the model’s weights, we achieve reasoning capability by adjusting how we prompt, sample, or aggregate LLM's outputs.\n",
    "\n",
    "In this section, we’ll explore several inference-time strategies that improve reasoning quality using a non-reasoning base model. You will experiment with and compare methods such as:\n",
    "\n",
    "- Few-shot Chain-of-Thought (CoT)\n",
    "- Zero-shot CoT\n",
    "- Self-consistency\n",
    "- Sequential revision\n",
    "- Tree-of-Thoughts (ToT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d499a",
   "metadata": {},
   "source": [
    "### 2.1: Few‑Shot CoT\n",
    "Few-shot prompting helps a model reason by showing one or multiple examples before asking a new question. By observing the pattern of reasoning and final answers, the model learns how to structure its own reasoning process on the new input.\n",
    "\n",
    "In this exercise, you will create a prompt that includes a few example Q&A pairs demonstrating step-by-step reasoning. Then, you will feed a new question and see the model’s output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "173d73f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overfitting and underfitting are common issues in machine learning modeling:\n",
      "\n",
      "**Underfitting**: When a model is too simple to capture the underlying patterns in the data, resulting in poor performance. The model fails to learn the essential features of the data, leading to high error rates.\n",
      "\n",
      "**Overfitting**: Conversely, when a model is too complex and memorizes the training data, fitting it extremely closely but failing to generalize well to new, unseen data. This results in good performance on the training set but poor performance on the test set, due to capturing random noise instead of underlying patterns.\n",
      "\n",
      "Both underfitting and overfitting are undesirable outcomes; the optimal scenario is to find a model with sufficient complexity to capture important features while avoiding excessive memorization of noise.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Write a few examples showing reasoning steps\n",
    "# Step 2: Write your new question\n",
    "# Step 3: Concatenate examples + new question into a single prompt\n",
    "# Step 4: Call your Ollama or OpenAI client to get a response from llama3.2:3b # e.g., client.chat.completions.create(...)\n",
    "# Step 5: Print the final answer\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "examples = [\n",
    "    \"Q: Why is the sky blue?\\nA: Because molecules scatter shorter wavelengths more.\",\n",
    "    \"Q: What is 2+2?\\nA: 4\",\n",
    "]\n",
    "new_question = \"Explain overfitting vs underfitting briefly.\"\n",
    "prompt = \"\\n\\n\".join(examples + [f\"Q: {new_question}\\nA:\"])\n",
    "client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "resp = client.chat.completions.create(model=\"llama3.2:3b\", messages=[{\"role\":\"user\",\"content\":prompt}])\n",
    "print(resp.choices[0].message.content.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e29d9",
   "metadata": {},
   "source": [
    "### (Optional) Few-shot CoT on GPT2\n",
    "GPT-2 is a pre-trained language model without instruction tuning. It continues text rather than answering questions. In this section, you'll try the exact same CoT pattern on GPT-2 and observe what happens. The goal is to test whether few-shot CoT alone can elicit structured reasoning from a non-chat LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8af711f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Output 1 ===\n",
      "Question: What is 12+7?\n",
      "Steps: 12+7=19\n",
      "Final: 19\n",
      "\n",
      "Question: A car goes 60 km in 2 h. Speed?\n",
      "Steps: speed=60/2=30 km/h\n",
      "Final: 30 km/h\n",
      "\n",
      "Question: If x=5 and y=3, what is x^2 + y?\n",
      "Steps:\n",
      "\n",
      "Final: x=5/2=5\n",
      "\n",
      "Question: If x=5 and y=3, what is x^2 + y?\n",
      "\n",
      "Steps:\n",
      "\n",
      "Final: x=5/2=5\n",
      "\n",
      "Question: If x=5 and y=3, what is x^2 + y?\n",
      "\n",
      "Steps:\n",
      "\n",
      "Final: x=5/2=\n",
      "\n",
      "=== Output 2 ===\n",
      "Question: What is 12+7?\n",
      "Steps: 12+7=19\n",
      "Final: 19\n",
      "\n",
      "Question: A car goes 60 km in 2 h. Speed?\n",
      "Steps: speed=60/2=30 km/h\n",
      "Final: 30 km/h\n",
      "\n",
      "Question: If x=5 and y=3, what is x^2 + y?\n",
      "Steps:\n",
      "\n",
      "Final:\n",
      "\n",
      "Question: When you are at the top of a hill like a mountain, this causes you to walk up it, at the bottom it causes you to walk down it, and, at the top the two things start to cross each other. How does that happen?\n",
      "\n",
      "=== Output 3 ===\n",
      "Question: What is 12+7?\n",
      "Steps: 12+7=19\n",
      "Final: 19\n",
      "\n",
      "Question: A car goes 60 km in 2 h. Speed?\n",
      "Steps: speed=60/2=30 km/h\n",
      "Final: 30 km/h\n",
      "\n",
      "Question: If x=5 and y=3, what is x^2 + y?\n",
      "Steps:\n",
      "\n",
      "Final: x=5/3 = 3.5\n",
      "\n",
      "Question: 1. What is x=4?\n",
      "\n",
      "Steps:\n",
      "\n",
      "Final: y=4/3 = 4.5\n",
      "\n",
      "Question: 1. What is x=5?\n",
      "\n",
      "Steps:\n",
      "\n",
      "Final: 5/3 = 5.5\n",
      "\n",
      "Question: 1. What is x=6?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Step 1: Load GPT-2 text-generation from huggingface (https://huggingface.co/docs/transformers/en/model_doc/gpt2)\n",
    "# Step 2: Write 1–2 few-shot reasoning examples (short, explicit steps + final answer in your own unique format)\n",
    "# Step 3: Append a new test question after the examples to form one prompt string\n",
    "# Step 4: Generate 1–3 completions with different decoding settings (e.g., greedy vs. top-k)\n",
    "# Step 5: Print raw outputs; check if steps are followed and if the final answer is correct\n",
    "\n",
    "gen = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "examples = [\n",
    "    \"Question: What is 12+7?\\nSteps: 12+7=19\\nFinal: 19\",\n",
    "    \"Question: A car goes 60 km in 2 h. Speed?\\nSteps: speed=60/2=30 km/h\\nFinal: 30 km/h\",\n",
    "]\n",
    "question = \"If x=5 and y=3, what is x^2 + y?\"\n",
    "prompt = \"\\n\\n\".join(examples + [f\"Question: {question}\\nSteps:\", \"Final:\"])\n",
    "\n",
    "outs = []\n",
    "outs.append(gen(prompt, max_new_tokens=80, do_sample=False)[0][\"generated_text\"])  # greedy\n",
    "outs.append(gen(prompt, max_new_tokens=80, do_sample=True, top_k=50, temperature=0.9)[0][\"generated_text\"])  # top-k\n",
    "outs.append(gen(prompt, max_new_tokens=80, do_sample=True, top_p=0.9, temperature=0.8)[0][\"generated_text\"])  # nucleus\n",
    "\n",
    "for i, o in enumerate(outs, 1):\n",
    "    print(f\"=== Output {i} ===\\n{o}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adee0e7",
   "metadata": {},
   "source": [
    "### 2.2: Zero‑Shot Chain‑of‑Thought\n",
    "Zero-shot CoT encourages the model to reason without examples by adding a short cue such as “Let’s think step by step.” This simple phrase often activates the model’s latent reasoning ability even when no demonstrations are provided. It serves as a baseline to compare with few-shot and other inference-time scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c444eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find out the total number of apples in the store, let's break it down:\n",
      "\n",
      "1. The store initially had 25 apples.\n",
      "2. They sold 7 apples.\n",
      "3. To account for the sale, we subtract 7 from 25: 25 - 7 = 18\n",
      "4. Then, they received 15 more apples.\n",
      "\n",
      "To find the new total, we simply add the remaining apples (after the sale) to the newly arrived apples:\n",
      "\n",
      "18 + 15 = 33\n",
      "\n",
      "So, now the store has a total of 33 apples.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Step 1: Write the question and a zero-shot CoT cue (e.g., \"Let's think step by step.\")\n",
    "# Step 2: Build a single prompt string that includes brief role guidance plus the question\n",
    "# Step 3: Call your Ollama or OpenAI client to get a response from llama3.2:3b  # e.g., client.chat.completions.create(...)\n",
    "# Step 4: Print the chain and the final answer\n",
    "\n",
    "question = \"If a store has 25 apples and sells 7, then gets 15 more, how many apples are there now?\"\n",
    "role = \"You are a helpful assistant that solves problems using clear reasoning.\"\n",
    "cot_cue = \"Let's think step by step.\"\n",
    "prompt = f\"{role}\\n\\n{cot_cue}\\n\\n{question}\"\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "resp = client.chat.completions.create(model=\"llama3.2:3b\", messages=[{\"role\":\"user\",\"content\":prompt}])\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686708da",
   "metadata": {},
   "source": [
    "### 2.3 Self‑Consistency\n",
    "Self-consistency enhances reasoning accuracy by sampling multiple independent reasoning paths for the same question instead of relying on a single deterministic answer. Each run may follow a slightly different logical chain, and the diversity helps correct individual mistakes. After generating several reasoning traces, you then aggregate the final answers using majority voting.\n",
    "\n",
    "This approach is especially useful when tasks involve multi-step reasoning or arithmetic, where single-path outputs may be incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e2fb325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Votes: 6\n",
      "Chosen answer: 12.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import re, collections\n",
    "\n",
    "client = OpenAI(api_key = \"ollama\", base_url = \"http://localhost:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "def cot_answer(question, temperature=1.0):\n",
    "    # Generate a step-by-step reasoning chain for the given question and extract the final answer.\n",
    "    prompt = f\"Let's think step by step.\\n\\n{question}\"\n",
    "    resp = client.chat.completions.create(model=MODEL, messages=[{\"role\":\"user\",\"content\":prompt}], temperature=temperature)\n",
    "    text = resp.choices[0].message.content\n",
    "    # Look for the last number in the reasoning chain, prioritizing numbers near the end\n",
    "    matches = re.findall(r'(?:answer|result|equals?|is|so|therefore)\\s*[:=]?\\s*([0-9.]+)', text, re.I)\n",
    "    return text, matches[-1] if matches else None\n",
    "\n",
    "def self_consistent(question, n=10):\n",
    "    # Run multiple reasoning chains and select the most frequent final answer by majority voting.\n",
    "    answers = []\n",
    "    for i in range(n):\n",
    "        _, ans = cot_answer(question, temperature=0.8)\n",
    "        if ans:\n",
    "            answers.append(ans)\n",
    "    counter = collections.Counter(answers)\n",
    "    return counter.most_common(1)[0] if counter else (None, counter)\n",
    "\n",
    "\n",
    "question = \"What is the square root of 144?\"\n",
    "winner, counter = self_consistent(question)\n",
    "print(\"Votes:\", counter)\n",
    "print(\"Chosen answer:\", winner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bea715",
   "metadata": {},
   "source": [
    "### 2.4: Sequential Revision\n",
    "\n",
    "Sequential revision iteratively improves an answer by generating a first draft, critiquing it, and producing revised drafts that condition on prior answers. Each round should be short and focused, so improvements accumulate without drifting from the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07e5859d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Draft 1]\n",
      "Regularization, also known as L1 or L2 regularization, is a technique used to prevent overfitting in machine learning models. Overfitting occurs when a model becomes too complex and performs well on the training data but fails to generalize well to new, unseen data.\n",
      "\n",
      "Here's why regularization helps:\n",
      "\n",
      "1.  **Reduces capacity**: Regularization adds constraints to the model that reduce its capacity. This means less flexibility is available for the model to create complex patterns in the training data.\n",
      "2.  **Enforces sparsity or low dimensionality**: L1-regularized models (also known as Lasso) enforce sparsity by setting coefficients to zero unless they belong to a small subset of variables. In contrast, L2-regularized models (Ridge regression) do not set any variable to zero but penalize larger weights.\n",
      "3.  **Regularizes coefficients**: Coefficient vectors in the parameters are often treated as vectors and regularized together with other model terms to keep all of them at a certain level\n",
      "4.  **Prevents feature overfitting**: Regularization prevents models from creating non-important features that may be very complex patterns but does not improve the overall accuracy by reducing dimensionality and keeping the complexity under control.\n",
      "5.  **Enforces smoothness in weights**: Weight decay forces all of these weights to be close together, this reduces the risk of large spikes that help the model fit the training set better\n",
      "\n",
      "In summary, regularization limits the capacity of a machine learning model. By doing so, it prevents overfitting in several ways: \n",
      "\n",
      "*   Reduces overfitting\n",
      "*   2.     Enforces coefficient sparsity\n",
      "*   3.    Enforces weights to be small.\n",
      "*   4.    Regularize coefficients together with other terms to keep all of them at a certain level.\n",
      "\n",
      "[Draft 2]\n",
      "**Prevention of Overfitting through Regularization in Machine Learning**\n",
      "\n",
      "Regularization is a fundamental technique used in machine learning to prevent overfitting, which occurs when a model becomes too complex and achieves high accuracy on the training data but fails to generalize well to new, unseen data. In this context, regularization plays a crucial role in ensuring that models remain simple, interpretive, and robust.\n",
      "\n",
      "**Why Regularization Helps Prevent Overfitting**\n",
      "\n",
      "Regularization achieves its purpose by adding constraints to the model's parameters, thereby reducing its capacity for overfitting. The primary mechanisms behind regularization include:\n",
      "\n",
      "1.  **Reducing Capacity**: By limiting the model's capacity to create complex patterns in the training data, regularization prevents the model from becoming too specialized and underperforming on unseen data.\n",
      "\n",
      "2.  **Enforcing Sparsity or Low Dimensionality**: This technique forces certain coefficients (values assigned to variables) within a L1-regularized model (Lasso) to be zero unless they belong to an insignificant subset of variables. Similarly, L2-regularized models enforce smaller weights for all coefficients without explicitly setting them to zero.\n",
      "\n",
      "3.  **Regularizing Coefficients**: Regularization often applies constraints to the coefficient vectors within a model simultaneously with the actual function values to reduce overfitting. \n",
      "\n",
      "4.  **Preventing Feature Overfitting**: This technique limits how much a model can overfit into an excess number of features and ensures that feature significance plays a more important role in reducing data variability.\n",
      "\n",
      "5.  **Enforcing Smoothness in Weights**: Regularization's primary mechanism forces the magnitude of weights within coefficients to become lower so as not to generate \"spikes\" that may cause errors because model becomes very fit with overfitting due\n",
      "\n",
      "By employing regularization techniques, researchers can avoid or mitigate the issues associated with overfitting problems. Some common methods include:\n",
      "\n",
      "-   L1 Regularization\n",
      "    -   **Lasso**, which reduces capacity by setting coefficients to zero unless belonging \n",
      "    to small sets.\n",
      "\n",
      "-   L2 Regularization\n",
      "    *   Ridge and Laplace Regression, \n",
      "\n",
      "This approach is a powerful strategy in creating robust models that achieve good generalization.\n",
      "\n",
      "[Draft 3]\n",
      "**The Role of Regularization in Preventing Overfitting in Machine Learning**\n",
      "\n",
      "Regularization is a crucial technique used in machine learning to mitigate the issue of overfitting, which occurs when a model becomes too complex and achieves high accuracy on the training data but fails to generalize well to new, unseen data. In this context, regularization plays a vital role in ensuring that models remain simple, interpretable, and robust.\n",
      "\n",
      "**Why Regularization Helps Prevent Overfitting**\n",
      "\n",
      "Regularization works by imposing constraints on the model's parameters, thereby reducing its capacity for overfitting. The primary mechanisms behind regularization include:\n",
      "\n",
      "1.  **Reducing Model Capacity**: By limiting the model's ability to create complex patterns in the training data, regularization prevents the model from becoming too specialized and underperforming on unseen data.\n",
      "\n",
      "2.  **Enforcing Sparsity or Low Dimensionality**: This technique forces certain coefficients (values assigned to variables) within a L1-regularized model (Lasso) to be zero unless they belong to an insignificant subset of variables. Similarly, L2-regularized models enforce smaller weights for all coefficients without explicitly setting them to zero.\n",
      "\n",
      "3.  **Regularizing Coefficient Vectors**: Regularization often applies constraints to the coefficient vectors within a model simultaneously with the actual function values to reduce overfitting.\n",
      "\n",
      "4.  **Preventing Feature Overfitting**: This technique limits how much a model can overfit into an excess number of features and ensures that feature significance plays a more important role in reducing data variability.\n",
      "\n",
      "5.  **Enforcing Smoothness in Weights**: Regularization's primary mechanism forces the magnitude of weights within coefficients to become lower so as not to generate \"spikes\" that may cause errors due to overfitting.\n",
      "\n",
      "**Types of Regularization Techniques**\n",
      "\n",
      "Several types of regularization techniques are used in machine learning, including:\n",
      "\n",
      "*   **L1 Regularization (Lasso)**: Reduces model capacity by setting coefficients to zero unless they belong to an insignificant subset of variables.\n",
      "*   **L2 Regularization (Ridge and Laplace Regression)**: Enforces smaller weights for all coefficients without explicitly setting them to zero.\n",
      "*   **Elastic Net Regularization**: Combines L1 and L2 regularization techniques.\n",
      "\n",
      "**Benefits of Regularization**\n",
      "\n",
      "Regularization offers several benefits, including:\n",
      "\n",
      "*   **Prevention of Overfitting**: Regularization helps prevent overfitting by limiting the model's capacity for complex patterns.\n",
      "*   **Improved Generalization**: By reducing overfitting, regularization improves model generalization to new data.\n",
      "*   **Interpretability and Sparsity**: Regularization techniques like L1 regularization can lead to sparse models with interpretable coefficients.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "In conclusion, regularization is a crucial technique used in machine learning to mitigate the issue of overfitting. By imposing constraints on the model's parameters, regularization reduces its capacity for complex patterns and prevents overfitting. Understanding the different types of regularization techniques and their benefits is essential for building robust models that achieve good generalization and interpretability.\n",
      "\n",
      "=== Final Answer ===\n",
      "**The Role of Regularization in Preventing Overfitting in Machine Learning**\n",
      "\n",
      "Regularization is a crucial technique used in machine learning to mitigate the issue of overfitting, which occurs when a model becomes too complex and achieves high accuracy on the training data but fails to generalize well to new, unseen data. In this context, regularization plays a vital role in ensuring that models remain simple, interpretable, and robust.\n",
      "\n",
      "**Why Regularization Helps Prevent Overfitting**\n",
      "\n",
      "Regularization works by imposing constraints on the model's parameters, thereby reducing its capacity for overfitting. The primary mechanisms behind regularization include:\n",
      "\n",
      "1.  **Reducing Model Capacity**: By limiting the model's ability to create complex patterns in the training data, regularization prevents the model from becoming too specialized and underperforming on unseen data.\n",
      "\n",
      "2.  **Enforcing Sparsity or Low Dimensionality**: This technique forces certain coefficients (values assigned to variables) within a L1-regularized model (Lasso) to be zero unless they belong to an insignificant subset of variables. Similarly, L2-regularized models enforce smaller weights for all coefficients without explicitly setting them to zero.\n",
      "\n",
      "3.  **Regularizing Coefficient Vectors**: Regularization often applies constraints to the coefficient vectors within a model simultaneously with the actual function values to reduce overfitting.\n",
      "\n",
      "4.  **Preventing Feature Overfitting**: This technique limits how much a model can overfit into an excess number of features and ensures that feature significance plays a more important role in reducing data variability.\n",
      "\n",
      "5.  **Enforcing Smoothness in Weights**: Regularization's primary mechanism forces the magnitude of weights within coefficients to become lower so as not to generate \"spikes\" that may cause errors due to overfitting.\n",
      "\n",
      "**Types of Regularization Techniques**\n",
      "\n",
      "Several types of regularization techniques are used in machine learning, including:\n",
      "\n",
      "*   **L1 Regularization (Lasso)**: Reduces model capacity by setting coefficients to zero unless they belong to an insignificant subset of variables.\n",
      "*   **L2 Regularization (Ridge and Laplace Regression)**: Enforces smaller weights for all coefficients without explicitly setting them to zero.\n",
      "*   **Elastic Net Regularization**: Combines L1 and L2 regularization techniques.\n",
      "\n",
      "**Benefits of Regularization**\n",
      "\n",
      "Regularization offers several benefits, including:\n",
      "\n",
      "*   **Prevention of Overfitting**: Regularization helps prevent overfitting by limiting the model's capacity for complex patterns.\n",
      "*   **Improved Generalization**: By reducing overfitting, regularization improves model generalization to new data.\n",
      "*   **Interpretability and Sparsity**: Regularization techniques like L1 regularization can lead to sparse models with interpretable coefficients.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "In conclusion, regularization is a crucial technique used in machine learning to mitigate the issue of overfitting. By imposing constraints on the model's parameters, regularization reduces its capacity for complex patterns and prevents overfitting. Understanding the different types of regularization techniques and their benefits is essential for building robust models that achieve good generalization and interpretability.\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "def sequential_revision(question: str, max_steps: int = 3) -> str:\n",
    "    # Generate an initial draft answer, then iteratively refine it by conditioning each revision on the previous one.\n",
    "    client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "    # Step 1: Ask the model to produce the first draft for the given question\n",
    "    draft = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\":\"user\",\"content\":f\"{question}\"}]\n",
    "    ).choices[0].message.content\n",
    "    print(f\"[Draft 1]\\n{draft}\\n\")\n",
    "\n",
    "    # Step 2: Loop for max_steps-1 times, each time feeding the last draft back to the model with a request to revise\n",
    "    for i in range(2, max_steps + 1):\n",
    "        revised = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\":\"user\",\"content\":f\"{question}\"},\n",
    "                {\"role\":\"assistant\",\"content\":draft},\n",
    "                {\"role\":\"user\",\"content\":\"Please revise and improve this answer. Make it clearer and more thorough.\"}\n",
    "            ]\n",
    "        ).choices[0].message.content\n",
    "        print(f\"[Draft {i}]\\n{revised}\\n\")\n",
    "        draft = revised\n",
    "\n",
    "    return draft\n",
    "    # Step 3: Print each draft to observe how the answer evolves\n",
    "    # Step 4: Return the final improved draft\n",
    "\n",
    "\n",
    "# Step 1: Define a question that benefits from multi-step reasoning\n",
    "question = \"Explain why regularization helps prevent overfitting in machine learning.\"\n",
    "# Step 2: Call sequential_revision(question, max_steps)\n",
    "final_answer = sequential_revision(question, max_steps=3)\n",
    "# Step 3: Print the final output\n",
    "print(\"=== Final Answer ===\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9319ee8",
   "metadata": {},
   "source": [
    "### 2.5 Tree‑of‑Thoughts\n",
    "Tree-of-Thoughts reframes reasoning as a search process rather than a single forward chain.\n",
    "Instead of producing one linear sequence of thoughts, the model generates multiple candidate thoughts at each step, evaluates their promise, and then expands only the best few. This allows exploration of different reasoning paths before committing to a final answer, similar to how humans brainstorm, prune, and refine ideas.\n",
    "\n",
    "\n",
    "In this section, you’ll experiment with two simplified versions of ToT:\n",
    "1. Word Ladder puzzle solver: a small example where each “thought” is a candidate word transition.\n",
    "2. Generic ToT search (depth 2, width 2): a minimal logic to expand, evaluate, and select reasoning branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d047801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hit', 'hot', 'dot', 'dog', 'cog']\n"
     ]
    }
   ],
   "source": [
    "###### Word Ladder Puzzle ##########\n",
    "\n",
    "def neighbors(word, vocabulary):\n",
    "    # Generate all valid one-letter mutations of 'word' that exist in 'vocabulary' and return them.\n",
    "    result = []\n",
    "    for i in range(len(word)):\n",
    "        for char in 'abcdefghijklmnopqrstuvwxyz':\n",
    "            candidate = word[:i] + char + word[i+1:]\n",
    "            if candidate in vocabulary and candidate != word:\n",
    "                result.append(candidate)\n",
    "    return result\n",
    "\n",
    "\n",
    "def tree_of_thought(start, goal, vocab, max_depth=5, beam_width=4):\n",
    "    # Search over partial thoughts (paths) using a small beam.\n",
    "    # Step 1: Initialize the frontier with a single path [start]\n",
    "    # Step 2: For each depth, expand each path by one neighbor from 'neighbors'\n",
    "    # Step 3: Score paths by edit distance between last word and 'goal' (smaller is better)\n",
    "    # Step 4: Keep the top 'beam_width' paths and stop early if any reaches 'goal'\n",
    "    # Step 5: Return the best goal-reaching path or None\n",
    "    from difflib import SequenceMatcher\n",
    "\n",
    "    frontier = [[start]]\n",
    "\n",
    "    for depth in range(max_depth):\n",
    "        candidates = []\n",
    "        for path in frontier:\n",
    "            if path[-1] == goal:\n",
    "                return path\n",
    "            for neighbor in neighbors(path[-1], vocab):\n",
    "                if neighbor not in path:\n",
    "                    candidates.append(path + [neighbor])\n",
    "\n",
    "        if not candidates:\n",
    "            return None\n",
    "\n",
    "        scores = [(path, len(path[-1]) - sum(a==b for a,b in zip(path[-1], goal)))\n",
    "                  for path in candidates]\n",
    "        scores.sort(key=lambda x: x[1])\n",
    "        frontier = [path for path, _ in scores[:beam_width]]\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "vocab = {\"hit\",\"dot\",\"cog\",\"log\",\"dog\",\"lot\",\"lit\",\"hot\"}\n",
    "print(tree_of_thought(\"hit\", \"cog\", vocab)) # one candidate solution: ['hit', 'hot', 'dot', 'dog', 'cog']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89067302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best solution (score 9):\n",
      "**Weekend Science Workshop Plan for 12-Year-Olds**\n",
      "\n",
      "**Workshop Title:** \"Curious Minds: Exploring the World of Science\"\n",
      "\n",
      "**Objective:** To provide an engaging and interactive science experience for 12-year-olds, fostering curiosity, critical thinking, and problem-solving skills.\n",
      "\n",
      "**Duration:** Saturday and Sunday (6 hours each day)\n",
      "\n",
      "**Age Group:** 12-year-olds\n",
      "\n",
      "**Number of Participants:** 10-15 students\n",
      "\n",
      "**Workshop Schedule:**\n",
      "\n",
      "**Day 1 (Saturday):**\n",
      "\n",
      "9:00 am - 9:30 am: Welcome and Introduction\n",
      "   - Icebreaker games to get participants comfortable with each other.\n",
      "   - Brief overview of the workshop's objectives and agenda.\n",
      "\n",
      "9:30 am - 10:30 am: **Physics in Action**\n",
      "   - Hands-on activities exploring motion, forces, and energy (e.g., marble runs, ball toss).\n",
      "   - Introduce basic physics concepts through experiments and discussions.\n",
      "\n",
      "10:30 am - 10:50 am: Break\n",
      "   - Snack time and opportunity for participants to ask questions or share experiences.\n",
      "\n",
      "10:50 am - 12:00 pm: **Materials Science**\n",
      "   - Investigate the properties of everyday materials (e.g., wood, metal, plastic).\n",
      "   - Use simple experiments to demonstrate material science principles (e.g., magnetism, conductivity).\n",
      "\n",
      "12:00 pm - 1:00 pm: Lunch Break\n",
      "   - Encourage participants to share their findings and discuss observations.\n",
      "\n",
      "1:00 pm - 2:30 pm: **Life Science**\n",
      "   - Explore the human body through interactive stations (e.g., anatomy, blood circulation).\n",
      "   - Introduce basic concepts of ecology, conservation, and environmental science.\n",
      "\n",
      "2:30 pm - 3:00 pm: Break\n",
      "   - Snack time and opportunity for participants to ask questions or share experiences.\n",
      "\n",
      "**Day 2 (Sunday):**\n",
      "\n",
      "9:00 am - 10:00 am: **STEM Challenges**\n",
      "   - Design and build projects that require physics, materials science, and life scientific principles (e.g., bridge building, roller coaster).\n",
      "   - Foster creativity and teamwork through peer feedback and discussion.\n",
      "\n",
      "10:00 am - 11:30 am: **Environmental Science**\n",
      "   - Investigate the impact of human actions on the environment.\n",
      "   - Develop strategies for reducing waste, conserving resources, and promoting sustainability.\n",
      "\n",
      "11:30 am - 12:00 pm: Break\n",
      "   - Snack time and opportunity for participants to ask questions or share experiences.\n",
      "\n",
      "**Next Thoughts:**\n",
      "\n",
      "1. **Assessment and Evaluation:** Consider incorporating a simple assessment or evaluation method to gauge participants' understanding of the concepts learned during the workshop.\n",
      "2. **Incorporating Technology:** Integrate technology-enhanced activities, such as simulations, videos, or interactive games, to enhance engagement, accessibility, and learning outcomes.\n",
      "\n",
      "**Continuing with the Current Thinking:**\n",
      "\n",
      "To further develop this plan, explore incorporating more hands-on experiments, group projects, or guest speakers to provide a rich and diverse science experience for participants. Here are two potential next thoughts to continue developing the \"Curious Minds: Exploring the World of Science\" weekend science workshop plan:\n",
      "\n",
      "**Next Thought 1:** **Incorporate Real-World Applications**\n",
      "\n",
      "To make the scientific concepts more relevant and engaging for 12-year-olds, consider incorporating real-world applications and case studies into the workshop. For example:\n",
      "\n",
      "* In the physics segment, use examples of inventions or technologies that rely on principles learned during the workshop (e.g., roller coasters, bicycles).\n",
      "* In the life science segment, explore how scientific concepts are applied in medicine, conservation, or environmental management (e.g., DNA testing, wildlife conservation).\n",
      "* In the STEM challenges, encourage participants to design and build projects that address real-world problems or issues (e.g., designing a prosthetic limb, creating a sustainable energy solution).\n",
      "\n",
      "By incorporating real-world applications, you can help participants see the relevance and importance of science in their daily lives.\n",
      "\n",
      "**Next Thought 2:** **Create a Sense of Community and Connection**\n",
      "\n",
      "To foster a sense of community and connection among participants, consider implementing activities that promote collaboration, peer feedback, and reflection. For example:\n",
      "\n",
      "* Pair participants with a \"buddy\" partner for the duration of the workshop to encourage sharing and learning from one another.\n",
      "* Incorporate \"sharing circles\" or small group discussions where participants can share their own experiences and insights related to scientific concepts learned during the workshop.\n",
      "* Encourage participants to create a \"Science Journal\" where they can record observations, ask questions, and reflect on their learning throughout the weekend.\n",
      "\n",
      "By creating a sense of community and connection among participants, you can help foster a deeper understanding of scientific concepts and promote long-term engagement with science.\n"
     ]
    }
   ],
   "source": [
    "###### Generic ToT Search ##########\n",
    "\n",
    "import re\n",
    "\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "def propose_thoughts(question, state, k=2):\n",
    "    # Propose up to k next “thoughts” that extend the current partial solution/state.\n",
    "    # Steps: build a short prompt with problem + current state; call your client with n=k. Then return a list of stripped strings (≤ k).\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "    prompt = f\"Question: {question}\\nCurrent thinking: {state}\\nPropose {k} next thoughts to continue solving this.\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\":\"user\", \"content\":prompt}],\n",
    "        n=k,\n",
    "        temperature=0.9\n",
    "    )\n",
    "    return [choice.message.content.strip() for choice in resp.choices]\n",
    "\n",
    "\n",
    "def score_state(question, state):\n",
    "    # Score how promising a partial solution is on a 1–10 scale (higher is better).\n",
    "    # Steps: build a rating prompt; call the model; parse the first integer 1–10;\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "    prompt = f\"Question: {question}\\nProposed solution: {state}\\nRate how promising this is from 1-10:\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\":\"user\", \"content\":prompt}]\n",
    "    )\n",
    "    match = re.search(r'\\b([1-9]|10)\\b', resp.choices[0].message.content)\n",
    "    return int(match.group(1)) if match else 5\n",
    "\n",
    "\n",
    "def tree_of_thoughts(question, depth=2, width=2):\n",
    "    # Run a tiny ToT search: expand states with propose_thoughts, score with score_state, keep top-k at each depth.\n",
    "    # Steps: initialize frontier=[(\"\", 0)]; for each depth, expand each state with k=width thoughts; score each; sort by score desc; keep top 'width'; return best state and score.\n",
    "    frontier = [(\"\", 0)]\n",
    "\n",
    "    for d in range(depth):\n",
    "        candidates = []\n",
    "        for state, _ in frontier:\n",
    "            thoughts = propose_thoughts(question, state, k=width)\n",
    "            for thought in thoughts:\n",
    "                new_state = state + \" \" + thought if state else thought\n",
    "                candidates.append((new_state, score_state(question, new_state)))\n",
    "\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        frontier = candidates[:width]\n",
    "\n",
    "    return frontier[0] if frontier else (\"\", 0)\n",
    "\n",
    "\n",
    "question = \"Design a plan for a weekend science workshop for 12-year-olds.\"\n",
    "solution, score = tree_of_thoughts(question)\n",
    "\n",
    "print(f\"Best solution (score {score}):\\n{solution}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dc38f6",
   "metadata": {},
   "source": [
    "---  \n",
    "# 3‑ Training Models for Reasoning\n",
    "\n",
    "### 3.1: CoT Training\n",
    "Chain-of-Thought (CoT) training conditions the model on explicit rationales during fine-tuning. Instead of teaching the model to output only the final answer, we train on (question, rationale, answer) so the model learns to internalize multi-step reasoning patterns. A practical recipe is STaR (Self-Taught Reasoner), which uses a stronger teacher model to bootstrap rationales that a smaller student can learn from.\n",
    "\n",
    "For tasks that require multi-hop reasoning, models fine-tuned on rationales often achieve higher accuracy and are more stable at inference time than models trained on direct answers only. \n",
    "\n",
    "Training a full language model is beyond the scope of this notebook, but here is the high-level workflow followed by a short pseudocode:\n",
    "- Collect questions: Prepare a dataset of questions and correct answers.\n",
    "- Generate rationales: Use a strong LLM to produce step-by-step reasoning ending with the correct answer.\n",
    "- Filter and clean: Discard incorrect or low-quality rationales.\n",
    "- Prepare training data: Format triples (question, rationale, answer) for supervised fine-tuning.\n",
    "- Fine-tune: Fine-tune the LLM on rationales.\n",
    "- Iterate: Refine prompts, improve data quality, and retrain for stronger reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb7cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode (STaR loop)\n",
    "# for round in 1 ... iters:\n",
    "    # STEP 1: self-generate reasoning (teacher creates rationale + answer)\n",
    "    # STEP 2: keep only correct, high-quality traces\n",
    "    # STEP 3: fine-tune student on (question, rationale, answer) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b53c70",
   "metadata": {},
   "source": [
    "### 3.2: ORM vs PRM + RL\n",
    "Training a Reward Model (RM) allows large language models to be improved through reinforcement learning (RL). Instead of fine-tuning directly on examples, we train a separate model that can score or rank model outputs, and use those scores as feedback signals to refine the policy model.\n",
    "\n",
    "Two main reward modeling approaches are ORM (predicts a scalar reward for the final answer) and PRM (evaluates the reasoning steps instead of just the outcome)\n",
    "\n",
    "\n",
    "\n",
    "| Approach | Typical loss | When to use |\n",
    "|-----------|-------------|-------------|\n",
    "|*Outcome Reward Model* | Predict scalar reward | Easy to collect training data using verifiers |\n",
    "|*Process Reward Model* | Predict rewards per step | Difficult to collect training data but more accurate |\n",
    "| *RLHF* | Use RM as reward in **RL** fine‑tuning | Aligns policy with human signals | Aligns model policy with human or synthetic preferences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595635aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for round = 1 ... iters:\n",
    "    # STEP 1:  Generate reasoning\n",
    "        # sample a minibatch of questions\n",
    "        # policy roll‑out (actions + log‑probs)\n",
    "    # STEP 2:  Score the trajectory\n",
    "        # ORM: scalar reward for the final answer / PRM: scalar reward for the thought process\n",
    "    # STEP 3:  Reinforce the policy (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a81a6",
   "metadata": {},
   "source": [
    "---  \n",
    "# 4‑ A Deep Research Agent\n",
    "\n",
    "A deep-research agent pairs a reasoning model (e.g., deepseek-r1) with external tools for web search and retrieval. We will follow the ReAct pattern: the model writes short thoughts, decides when to call tools, reads observations, and continues reasoning until it can answer or reaches a step limit.\n",
    "\n",
    "We now combine a **search tool** with a reasoning model (e.g., `deepseek-r1`) in a multi-step setup. We follow the *ReAct* pattern (reason → tool → observation):\n",
    "\n",
    "1. The model reasoins and decides to use tools\n",
    "2. The agent searches and feed condensed snippets back as context\n",
    "3. Iterate until the model answers or hits a step limit\n",
    "\n",
    "We use `AgentType.OPENAI_FUNCTIONS`, which hides the loop inside the LangChain agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd1e1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddgs import DDGS\n",
    "from langchain_community.tools import Tool\n",
    "\n",
    "def ddg_search(query: str, k: int = 5) -> str:\n",
    "    # Use DDGS to run a simple web search and return joined snippets.\n",
    "    results = []\n",
    "    with DDGS() as ddgs:\n",
    "        for r in ddgs.text(query, max_results=k):\n",
    "            results.append(r.get(\"body\", \"\"))\n",
    "    return \"\\n\\n\".join(results)\n",
    "\n",
    "search_tool = Tool(\n",
    "    name=\"DuckDuckGo Search\",\n",
    "    func=ddg_search,\n",
    "    description=\"Search the public web. Input: a plain English query. Returns: concatenated snippets.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "418a0d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fallback agent implementation...\n",
      "Based on the search results, the best resources to learn machine learning in 2025 include:\n",
      "\n",
      "1. **Andrew Ng’s Machine Learning Course**: A foundational course that introduces key concepts, though it uses Octave. It's recommended to learn Python alongside or after completing this course, as Python is crucial for most ML tasks.\n",
      "\n",
      "2. **Dataquest (Free Python Lessons)**: Offers interactive, browser-based Python lessons that are ideal for hands-on practice and building a strong programming foundation for machine learning.\n",
      "\n",
      "3. **Keras**: Particularly suited for quickly prototyping neural networks. It's easy to use and is a go-to library for modern deep learning projects.\n",
      "\n",
      "4. **PyTorch and TensorFlow Courses**:\n",
      "   - **PyTorch Full Course**: A comprehensive series on YouTube, designed for deep learning and machine learning.\n",
      "   - **TensorFlow Tutorial Series**: Covers installation, environment setup, and advanced concepts like Neural Networks and the TensorFlow 2.0 platform.\n",
      "\n",
      "5. **Reddit Communities**: r/learnmachinelearning provides support, shared resources, and a space to ask questions. For ML research, r/machinelearning is useful, while r/engineeringresumes helps with resumes for ML engineers.\n",
      "\n",
      "6. **Comprehensive Resource Lists**: Websites compiling over 50 machine learning resources for self-study will offer further guidance on tools, tutorials, and courses.\n",
      "\n",
      "For structured learning, start with Andrew Ng’s course and Python fundamentals through Dataquest. Then, practice with deep learning frameworks like PyTorch and TensorFlow, using platforms like YouTube tutorials. Engage with online communities to stay updated and resolve doubts.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from langchain_ollama import ChatOllama\n",
    "except ImportError:\n",
    "    from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "try:\n",
    "    from langchain.agents import initialize_agent, AgentType\n",
    "    USE_AGENT = True\n",
    "except ImportError:\n",
    "    USE_AGENT = False\n",
    "    print(\"Using fallback agent implementation...\")\n",
    "\n",
    "MODEL = \"deepseek-r1:8b\"\n",
    "question = \"What are the best resources to learn machine learning in 2025?\"\n",
    "\n",
    "# Step 1: Initialize the reasoning model via ChatOllama\n",
    "llm = ChatOllama(model=MODEL, base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Step 2: Build the agent with tool access (DuckDuckGo Search) and function-calling interface\n",
    "if USE_AGENT:\n",
    "    agent = initialize_agent([search_tool], llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "    result = agent.invoke({\"input\": question})\n",
    "    print(result[\"output\"])\n",
    "else:\n",
    "    # Fallback: direct call with search\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "\n",
    "    # First search\n",
    "    search_results = ddg_search(\"best machine learning resources 2025\", k=5)\n",
    "\n",
    "    # Ask model with context\n",
    "    prompt = f\"Based on the following search results, answer: {question}\\n\\nSearch results:\\n{search_results}\"\n",
    "    resp = client.chat.completions.create(model=MODEL, messages=[{\"role\":\"user\",\"content\":prompt}])\n",
    "    print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1c3f7",
   "metadata": {},
   "source": [
    "# Optional (Multi-agent Deep Research)\n",
    "Instead of a single multi-step agent, you can design multiple collaborating agents such as a Planner, Searcher, Summarizer, and Verifier that pass information and refine each other’s outputs. This setup improves robustness, diversity of reasoning, and division of labor.\n",
    "\n",
    "Try building a simple setup with 2–3 agents that share goals and messages, for example Planner → Researcher → Writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59abf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_research(query, n=3):\n",
    "    # Run n independent research runs in parallel and return their answers.\n",
    "    # Steps: use ThreadPoolExecutor; submit n calls to your agent/search pipeline; gather results in order.\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE\n",
    "    \"\"\"\n",
    "\n",
    "answers = parallel_research(\"What are the best resources to learn ML in 2025?\")\n",
    "for i,a in enumerate(answers,1):\n",
    "    print(f\"[Run {i}] {a[:200]}…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9507d0a4",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "* Practised various inference‑time reasoning methods\n",
    "* Gained intuition about training reasoning models\n",
    "* You have built a **deep-research agent**: reasoning model like deep-seek r1 + ReAct-style agent + tool use (web search)\n",
    "* Try adding more tools, and extending the deep-research to a multi-agent system: many agents researching web in parallel.\n",
    "\n",
    "\n",
    "👏 **Great job!** Take a moment to celebrate. The techniques you implemented here power many production agents and chatbots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
